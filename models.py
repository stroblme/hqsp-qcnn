from tensorflow import keras
from tensorflow.keras.layers  import Conv2D, MaxPooling2D, Dense,Flatten, GRU, BatchNormalization, Conv1D, Dropout, Bidirectional,MaxPooling1D, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import RMSprop, SGD
from tensorflow.keras import layers as L
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Lambda, BatchNormalization, Conv1D, GRU, TimeDistributed, Activation, Dense, Flatten
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import categorical_crossentropy
import tensorflow as tf



def dense_Model(x, labels):
    """Initializes and returns a custom Keras model
    which is ready to be trained."""
    if len(x.shape) >= 3:
        h_feat,w_feat,ch_size = x.shape
        input_layer = keras.layers.Input(shape=(h_feat,w_feat,ch_size))
    else:
        h_feat,w_feat = x.shape
        input_layer = keras.layers.Input(shape=(h_feat,w_feat))
    model = keras.models.Sequential([
        input_layer,
        keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True),
        keras.layers.Flatten(),
        keras.layers.Dense(64),
        keras.layers.Dense(64),
        keras.layers.Dense(32),
        keras.layers.Dense(len(labels), activation="softmax")
    ])

    model.compile(
        optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model

# define cnn model
def cnn_Model(h_feat, w_feat, labels):
	model = Sequential()
	model.add(Conv2D(6, (2, 2), padding='valid', activation='relu', input_shape=(h_feat, w_feat, 1)))
	#model.add(MaxPooling2D((2, 2)))
	model.add(Flatten())
	model.add(Dense(len(labels), activation='softmax'))
	# compile model
	opt = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model

def attrnn_Model(x_in, labels, ablation = False):
    # simple LSTM
    rnn_func = L.LSTM
    use_Unet = True

    if len(x_in.shape) >= 3:
        h_feat,w_feat,ch_size = x_in.shape
        inputs = keras.layers.Input(shape=(h_feat, w_feat, ch_size))
    else:
        h_feat, w_feat = x_in.shape
        inputs = keras.layers.Input(shape=(h_feat, w_feat))

    inputs = L.Input(shape=(h_feat, w_feat, ch_size))

    if ablation == True:
        x = L.Conv2D(4, (1, 1), strides=(2, 2), activation='relu', padding='same', name='abla_conv')(inputs)
        x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)
    else:
        x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(inputs)

    # note that Melspectrogram puts the sequence in shape (batch_size, melDim, timeSteps, 1)
    # we would rather have it the other way around for LSTMs

    x = L.Permute((2, 1, 3))(x)

    if use_Unet == True:
        x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)
        up = L.BatchNormalization()(x)
        x = L.Conv2D(32, (5, 1), activation='relu', padding='same')(up)
        x = L.BatchNormalization()(x)
        x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)
        down = L.BatchNormalization()(x)
        merge = L.Concatenate(axis=3)([up,down])
        x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(merge)
        x = L.BatchNormalization()(x)
    else:
        x = L.Conv2D(10, (5, 1), activation='relu', padding='same')(x)
        x = L.BatchNormalization()(x)
        x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(x)
        x = L.BatchNormalization()(x)

    x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)

    x = L.Bidirectional(rnn_func(64, return_sequences=True)
                        )(x)  # [b_s, seq_len, vec_dim]
    x = L.Bidirectional(rnn_func(64, return_sequences=True)
                        )(x)  # [b_s, seq_len, vec_dim]

    xFirst = L.Lambda(lambda q: q[:, -1])(x)  # [b_s, vec_dim]
    query = L.Dense(128)(xFirst)

    # dot product attention
    attScores = L.Dot(axes=[1, 2])([query, x])
    attScores = L.Softmax(name='attSoftmax')(attScores)  # [b_s, seq_len]

    # rescale sequence
    attVector = L.Dot(axes=[1, 1])([attScores, x])  # [b_s, vec_dim]

    x = L.Dense(64, activation='relu')(attVector)
    x = L.Dense(32)(x)

    output = L.Dense(len(labels), activation='softmax', name='output')(x)

    model = Model(inputs=inputs, outputs=output)
    model.compile(
        optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    return model



class vqft_attrnn_model(Model):
    def __init__(self, x_in, labels, quantum_callback=None, ablation = False):

        # simple LSTM
        rnn_func = L.LSTM
        use_Unet = True

        if quantum_callback:
            assert(len(x_in.shape) == 1)
        elif len(x_in.shape) >= 3:
            h_feat,w_feat,ch_size = x_in.shape
            inputs = keras.layers.Input(shape=(h_feat, w_feat, ch_size)) # ch_size will always be 1
        else:
            # TODO: check if can actually run in this case
            h_feat, w_feat = x_in.shape
            inputs = keras.layers.Input(shape=(h_feat, w_feat))


        if ablation == True:
            if quantum_callback:
                q_in = VQFT(quantum_callback)(x_in)
                h_feat, w_feat = q_in.shape
                inputs = keras.layers.Input(shape=(h_feat, w_feat, 1))

            x = L.Conv2D(4, (1, 1), strides=(2, 2), activation='relu', padding='same', name='abla_conv')(inputs)
            x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)
        else:
            x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(inputs)

        # note that Melspectrogram puts the sequence in shape (batch_size, melDim, timeSteps, 1)
        # we would rather have it the other way around for LSTMs

        x = L.Permute((2, 1, 3))(x)

        if use_Unet == True:
            x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)
            up = L.BatchNormalization()(x)
            x = L.Conv2D(32, (5, 1), activation='relu', padding='same')(up)
            x = L.BatchNormalization()(x)
            x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)
            down = L.BatchNormalization()(x)
            merge = L.Concatenate(axis=3)([up,down])
            x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(merge)
            x = L.BatchNormalization()(x)
        else:
            x = L.Conv2D(10, (5, 1), activation='relu', padding='same')(x)
            x = L.BatchNormalization()(x)
            x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(x)
            x = L.BatchNormalization()(x)

        x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)

        x = L.Bidirectional(rnn_func(64, return_sequences=True))(x)  # [b_s, seq_len, vec_dim]
        x = L.Dropout(0.5)(x)
        # x = L.Dropout(0.5)(x)
        x = L.Bidirectional(rnn_func(64, return_sequences=True))(x)  # [b_s, seq_len, vec_dim]
        x = L.Dropout(0.5)(x)


        xFirst = L.Lambda(lambda q: q[:, -1])(x)  # [b_s, vec_dim]
        query = L.Dense(128)(xFirst)

        # dot product attention
        attScores = L.Dot(axes=[1, 2])([query, x])
        attScores = L.Softmax(name='attSoftmax')(attScores)  # [b_s, seq_len]

        # rescale sequence
        attVector = L.Dot(axes=[1, 1])([attScores, x])  # [b_s, vec_dim]

        x = L.Dense(64, activation='relu')(attVector)
        x = L.Dropout(0.5)(x)

        x = L.Dense(32)(x)
        # x = L.Dropout(0.5)(x)

        output = L.Dense(len(labels), activation='softmax', name='output')(x)

        model = super(vqft_attrnn_model, self).__init__(inputs=inputs, outputs=output)
        model.compile(
            # optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),
            optimizer=Adam(
                            learning_rate=0.0001,
                            beta_1=0.9,
                            beta_2=0.999,
                            epsilon=1e-07,
                            amsgrad=True,
                            name="Adam"),
            loss="categorical_crossentropy",
            metrics=["accuracy"],
        )

        return model


class VQFT(L.Layer):
    def __init__(self, qft_callback,**kwargs):
        self.qft_callback = qft_callback

        super(VQFT, self).__init__(**kwargs)
        
    def quantum_layer(self, x, **kwargs):
        return self.qft_callback(weights=self.w.numpy(), biases=self.b.numpy(), x=x, **kwargs)

    def get_config(self):
        config = super(VQFT, self).get_config()
        return config

    def call(self, inputs):
        
        if(tf.executing_eagerly()):
            final_output = []
            pred = self.quantum_layer(x=inputs) # note that inputs is a signal type here
            return tf.convert_to_tensor(pred)
        return inputs

    def build(self, input_shape):
        s = int(list(input_shape)[0])

        self.w = self.add_weight(
            shape=(s,), initializer="random_normal", trainable=True
        )
        self.b = self.add_weight(shape=(s,), initializer="zeros", trainable=True)


class CTCLayer(L.Layer):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.loss_fn = keras.backend.ctc_batch_cost

    def call(self, y_true, y_pred):
        # Compute the training-time loss value and add it
        # to the layer using `self.add_loss()`.
        batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
        input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
        label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")

        loss = self.loss_fn(y_true, y_pred, input_length, label_length)
        self.add_loss(loss)

        # At test time, just return the computed predictions
        return y_pred


def build_asr_model(h_feat, w_feat, ch_size = 1, volc_size = 26):
    use_cnn = False
    use_Unet = True
    # Inputs to the model
    input_img = L.Input(
        shape=(h_feat, w_feat, ch_size), name="speech", dtype="float32"
    )
    labels = L.Input(name="label", shape=(None,), dtype="float32")
    
    x = L.Permute((2, 1, 3))(input_img)
    if use_cnn != True:
        if use_Unet == True:
            x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)
            up = L.BatchNormalization()(x)
            x = L.Conv2D(32, (5, 1), activation='relu', padding='same')(up)
            x = L.BatchNormalization()(x)
            x = L.Conv2D(16, (5, 1), activation='relu', padding='same')(x)
            down = L.BatchNormalization()(x)
            merge = L.Concatenate(axis=3)([up,down])
            x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(merge)
            x = L.BatchNormalization()(x)
        else:
            x = L.Conv2D(10, (5, 1), activation='relu', padding='same')(x)
            x = L.BatchNormalization()(x)
            x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(x)
            x = L.BatchNormalization()(x)

        x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)
    else:
    # First conv block
        x = L.Conv2D(
            32,
            (3, 3),
            activation="relu",
            kernel_initializer="he_normal",
            padding="same",
            name="Conv1",
        )(x)
        x = L.MaxPooling2D((2, 2), name="pool1")(x)

        # Second conv block
        x = L.Conv2D(
            64,
            (3, 3),
            activation="relu",
            kernel_initializer="he_normal",
            padding="same",
            name="Conv2",
        )(x)
        x = L.MaxPooling2D((2, 2), name="pool2")(x)

        # We have used two max pool with pool size and strides 2.
        # Hence, downsampled feature maps are 4x smaller. The number of
        # filters in the last layer is 64. Reshape accordingly before
        # passing the output to the RNN part of the model
        new_shape = ((h_feat // 4), (w_feat // 4) * 64)
        x = L.Reshape(target_shape=new_shape, name="reshape")(x)

    # RNNs
    x = L.Bidirectional(L.LSTM(64, return_sequences=True))(x)
    x = L.Bidirectional(L.LSTM(64, return_sequences=True))(x)

    x = L.Dense(64, activation="relu", name="dense1")(x)
    x = L.Dense(32, activation="relu", name="dense11")(x)

    # Output layer
    x = L.Dense(volc_size, activation="softmax", name="dense2")(x)

    # Add CTC layer for calculating CTC loss at each step
    output = CTCLayer(name="ctc_loss")(labels, x)

    # Define the model
    model = keras.models.Model(
        inputs=[input_img, labels], outputs=output, name="asr_model_v1"
    )
    # Optimizer
    opt = keras.optimizers.Adam()
    # Compile the model and return
    model.compile(optimizer=opt)
    return model
